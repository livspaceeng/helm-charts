# Default values for myapp.
# This is a YAML-formatted file.
# Declare variables to be passed into your templates.
replicaCount: 1
MinReadySeconds: 5
MaxSurge: 1
MaxUnavailable: 0
GracePeriod: 30
ContainerPort:
  - name: app
    port: 8080
    servicePort: 80
    envoyPort: 8799
    envoyTimeout: 15s
    useHTTP2: false
    supportStreaming: false
    idleTimeout: 1800s
    # servicemonitor:
    #   enabled: false
    #   path: /abc
    #   scheme: 'http'
    #   interval: 30s
    #   scrapeTimeout: 10s
#      metricRelabelings:
#        - sourceLabels: [namespace]
#          regex: '(.*)'
#          replacement: myapp
#          targetLabel: target_namespace

  - name: app1
    port: 8090
    servicePort: 8080
    useGRPC: true

pauseForSecondsBeforeSwitchActive: 30
waitForSecondsBeforeScalingDown: 30
autoPromotionSeconds: 30

Spec:
  Affinity:
    Key:
    Values:


image:
  pullPolicy: IfNotPresent

restartPolicy: Always

ambassadorMapping:
  enabled: false
  # labels:
  #   key1: value1
  # prefix: /
  # ambassadorId: 1234
  # hostname: devtron.example.com
  # rewrite: /foo/
  # retryPolicy:
  #   retry_on: "5xx"
  #   num_retries: 10
  # cors:
  #   origins: http://foo.example,http://bar.example
  #   methods: POST, GET, OPTIONS
  #   headers: Content-Type
  #   credentials: true
  #   exposed_headers: X-Custom-Header
  #   max_age: "86400"
  # weight: 10
  # method: GET
  # extraSpec:
  #   method_regex: true
  #   headers:
  #     x-quote-mode: backend
  #     x-random-header: devtron
  # tls:
  #   context: httpd-context
  #   create: true
  #   secretName: httpd-secret
  #   hosts:
  #     - anything.example.info
  #     - devtron.example.com
  #   extraSpec:
  #     min_tls_version: v1.2

## Add a horizontal pod autoscaler
## ref: https://kubernetes.io/docs/tasks/run-application/horizontal-pod-autoscale/
## @param autoscaling.enabled Deploy a HorizontalPodAutoscaler object for the Kong deployment
## @param autoscaling.minReplicas Minimum number of replicas to scale back
## @param autoscaling.maxReplicas Maximum number of replicas to scale out
## @param autoscaling.metrics [array] Metrics to use when deciding to scale the deployment (evaluated as a template)
##
autoscaling:
  enabled: false
  MinReplicas: 1
  MaxReplicas: 2
  # Deprecated. Use autoscaling.metrics to define the autoscale metrics
  TargetCPUUtilizationPercentage: 90
  annotations: {}
  labels: {}
  metrics: []
#    - type: Resource
#      resource:
#        name: cpu
#        target:
#          averageUtilization: 90
#          type: Utilization
#    - type: Resource
#      resource:
#        name: memory
#        target:
#          averageUtilization: 90
#          type: Utilization
  behavior:
    scaleDown:
      stabilizationWindowSeconds: 180
      policies:
      - type: Percent
        value: 40
        periodSeconds: 60
    scaleUp:
      stabilizationWindowSeconds: 0
      policies:
      - type: Percent
        value: 100
        periodSeconds: 15
      - type: Pods
        value: 2
        periodSeconds: 15
      selectPolicy: Max
  extraMetrics: []
#    - external:
#        metricName: pubsub.googleapis.com|subscription|num_undelivered_messages
#        metricSelector:
#          matchLabels:
#            resource.labels.subscription_id: echo-read
#        targetAverageValue: "2"
#      type: External
#

kedaAutoscaling:
  enabled: false
  envSourceContainerName: ""  # Optional. Default: .spec.template.spec.containers[0]
  cooldownPeriod: 300  # Optional. Default: 300 seconds
  minReplicaCount: 1
  maxReplicaCount: 2
  idleReplicaCount: 0  # Optional. Must be less than minReplicaCount
  pollingInterval: 30  # Optional. Default: 30 seconds
  # The fallback section is optional. It defines a number of replicas to fallback to if a scaler is in an error state.
  fallback: {}  # Optional. Section to specify fallback options
    # failureThreshold: 3 # Mandatory if fallback section is included
    # replicas: 6
  advanced: {}
    # horizontalPodAutoscalerConfig: # Optional. Section to specify HPA related options
    # behavior: # Optional. Use to modify HPA's scaling behavior
    #   scaleDown:
    #     stabilizationWindowSeconds: 300
    #     policies:
    #     - type: Percent
    #       value: 100
    #       periodSeconds: 15
  triggers: []
  triggerAuthentication:
    enabled: false
    name: ""
    spec: {}
  authenticationRef: {}

# kedaHttpScaledObject:
#   enabled: false
#   minReplicaCount: 1
#   maxReplicaCount: 2
#   targetPendingRequests:
#   scaledownPeriod:
#   servicePort: 80 # port of the service (required)

secret:
  enabled: false

externalSecretsv2:
  enabled: false
  secrets: []
  # - stripe-api-key
  # - jwt-signing-key
  # - database-users/app_user
  # - redis-auth

service:
  type: ClusterIP
  enabled: true
#  name: "1234567890123456789012345678901234567890123456789012345678901234567890"
  annotations: {}
    # test1: test2
    # test3: test4

istio:
  enable: false
  gateway:
    enabled: false
    labels: {}
    annotations: {}
    host: ""
    tls:
      enabled: false
      secretName: ""
  virtualService:
    enabled: false
    labels: {}
    annotations: {}
    gateways: []
    hosts: []
    http: []
      # - match:
      #   - uri:
      #       prefix: /v1
      #   - uri:
      #       prefix: /v2
      #   timeout: 12
      #   headers:
      #     request:
      #       add:
      #         x-some-header: "value"
      #   retries:
      #     attempts: 2
      #     perTryTimeout: 3s
  destinationRule:
    enabled: false
    labels: {}
    annotations: {}
    subsets: []
    trafficPolicy: {}
  peerAuthentication:
    enabled: false
    labels: {}
    annotations: {}
    selector:
      enabled: false
    mtls:
      mode:
    portLevelMtls: {}
  requestAuthentication:
    enabled: false
    labels: {}
    annotations: {}
    selector:
      enabled: false
    jwtRules: []
  authorizationPolicy:
    enabled: false
    labels: {}
    annotations: {}
    action:
    provider: {}
    rules: []

api:
  enabled: false
  routes: []
  # - hosts: []
  #   # - api.beta.staging.livspace.com
  #   paths: []
  #   # - "/bouncer/*"
  #   service:
  #     port: 80
  #   proxy_rewrite:
  #     prefix: bouncer
  #     headers:
  #       set:
  #         X-Client-Id: xxx
  #         X-Client-Secret: zzz
  # loadbalancer: ewma
  # portLevelSettings:
  #   - port: 80
  #     scheme: http
  #   - port: 81
  #     scheme: grpc
  #     retries: 3
  # timeout:
  #   connect: 60s  #default
  #   read: 60s     #default
  #   send: 60s     #default

canary:
  enabled: false
  provider: "linkerd"
  labels: {}
  annotations: {}
  service:
    port: 80
    targetPort: 8080
    portDiscovery: false
    # application protocol (optional)
    # appProtocol:
    # Retry policy (optional)
    retries: {}
    #  attempts: 3
    #  perTryTimeout: 1s
    #  retryOn: "gateway-error,connect-failure,refused-stream"
    # HTTP match conditions (optional)
    match: []
    #  - uri:
    #      prefix: /
    # HTTP rewrite (optional)
    # rewriteUri: /
    # timeout (optional)
    # timeout:
  analysis:
    # schedule interval (default 60s)
    interval: 60s
    # max number of failed metric checks before rollback
    threshold: 5
    # max traffic percentage routed to canary
    # percentage (0-100)
    maxWeight: 50
    # canary increment step
    # percentage (0-100)
    stepWeight: 10
    metrics: []
    # - name: success-rate
    #   threshold: 99
    #   interval: 2m
    # - name: latency
    #   threshold: 500
    #   interval: 2m
  webhooks: {}
    # loadtest:
    #   enabled: false
    #   # load tester address
    #   url: http://flagger-loadtester.istio-system/

mountGToken: false

deployment:
  image:
    repository: livcr.io/livspace/app-name
    tag: 1-95af053
  strategy:
    rolling:
      maxSurge: 25%
      maxUnavailable: 1

EnvVariablesFromFieldPath: []
# - name: POD_NAME
#   fieldPath: metadata.name

EnvVariables: []
  # - name: FLASK_ENV
  #   value: qa

EnvVariablesFromSecretKeys: []
  # - name: ENV_NAME
  #   secretName: SECRET_NAME
  #   keyName: SECRET_KEY

EnvVariablesFromConfigMapKeys: []
  # - name: ENV_NAME
  #   configMapName: CONFIG_MAP_NAME
  #   keyName: CONFIG_MAP_KEY

LivenessProbe:
  Path: /
  port: 8080
  initialDelaySeconds: 20
  periodSeconds: 10
  successThreshold: 1
  timeoutSeconds: 5
  failureThreshold: 3
  httpHeaders: []
#    - name: Custom-Header
#      value: abc

ReadinessProbe:
  Path: /
  port: 8080
  initialDelaySeconds: 20
  periodSeconds: 10
  successThreshold: 1
  timeoutSeconds: 5
  failureThreshold: 3
  httpHeaders: []
#    - name: Custom-Header
#      value: abc

StartupProbe:
  Path: ""
  port: 8080
  initialDelaySeconds: 20
  periodSeconds: 10
  successThreshold: 1
  timeoutSeconds: 5
  failureThreshold: 3
  httpHeaders: []
  command: []
  tcp: false

prometheus:
  release: monitoring

metrics:
  enabled: false
  port: metrics
  interval: 30s
  scrapeTimeout: 10s

servicemonitor:
  additionalLabels: {}


prometheusRule:
  enabled: false
  additionalLabels: {}
  namespace: ""
#  rules:
#    # These are just examples rules, please adapt them to your needs
#    - alert: TooMany500s
#      expr: 100 * ( sum( nginx_ingress_controller_requests{status=~"5.+"} ) / sum(nginx_ingress_controller_requests) ) > 5
#      for: 1m
#      labels:
#        severity: critical
#      annotations:
#        description: Too many 5XXs
#        summary: More than 5% of the all requests did return 5XX, this require your attention
#    - alert: TooMany400s
#      expr: 100 * ( sum( nginx_ingress_controller_requests{status=~"4.+"} ) / sum(nginx_ingress_controller_requests) ) > 5
#      for: 1m
#      labels:
#        severity: critical
#      annotations:
#        description: Too many 4XXs
#        summary: More than 5% of the all requests did return 4XX, this require your attention
#

ingress:
  enabled: false
  className: ""
  labels: {}
  annotations: {}
#    nginx.ingress.kubernetes.io/rewrite-target: /
#    nginx.ingress.kubernetes.io/ssl-redirect: "false"
#    kubernetes.io/ingress.class: nginx
#    kubernetes.io/tls-acme: "true"
#    nginx.ingress.kubernetes.io/canary: "true"
#    nginx.ingress.kubernetes.io/canary-weight: "10"

  hosts:
    - host: chart-example1.local
      pathType: "ImplementationSpecific"
      paths:
        - /example1
    - host: chart-example2.local
      pathType: "ImplementationSpecific"
      paths:
        - /example2
        - /example2/healthz
  tls: []
  #  - secretName: chart-example-tls
  #    hosts:
  #      - chart-example.local

ingressInternal:
  enabled: false
  className: ""
  annotations: {}
 #    kubernetes.io/ingress.class: nginx
 #    kubernetes.io/tls-acme: "true"
 #    nginx.ingress.kubernetes.io/canary: "true"
 #    nginx.ingress.kubernetes.io/canary-weight: "10"

  hosts:
    - host: chart-example1.internal
      pathType: "ImplementationSpecific"
      paths:
        - /example1
    - host: chart-example2.internal
      pathType: "ImplementationSpecific"
      paths:
        - /example2
        - /example2/healthz
  tls: []
 #  - secretName: chart-example-tls
 #    hosts:
 #      - chart-example.local

winterSoldier:
  enabled: false
  apiVersion: pincher.devtron.ai/v1alpha1
  labels: {}
  annotations: {}
  type: Deployment
  timeRangesWithZone: {}
  # timeZone: "Asia/Kolkata"
  # timeRanges: []
  action: sleep
  targetReplicas: []
  fieldSelector: []
  # - AfterTime(AddTime(ParseTime({{metadata.creationTimestamp}}, '2006-01-02T15:04:05Z'), '5m'), Now())

networkPolicy:
  enabled: false
  # ingress defines rules for incoming traffic to your application pods
  # ingress:
  #   fromApps:
  #     - name: frontend                    # Name of the source app (matches the app label)
  #       labelKey: app                      # Optional: Label key to match (defaults to "app")
  #       namespace: frontend-ns             # Optional: Source namespace (omit for same namespace)
  #       httpPorts: [8080]                  # Optional: List of HTTP/TCP ports
  #       tcpPorts: [9090]                   # Optional: List of TCP ports
  #       udpPorts: []                       # Optional: List of UDP ports
  #     - name: api-gateway
  #       httpPorts: [3000]
  #     - name: monitoring-system
  #       labelKey: app.kubernetes.io/name
  #       namespace: monitoring
  #       tcpPorts: [8081]

  # egress defines rules for outgoing traffic from your application pods
  # egress:
  #   toApps:
  #     - name: database                    # Name of the target app (matches the app label)
  #       labelKey: app                      # Optional: Label key to match (defaults to "app")
  #       namespace: database-ns             # Optional: Target namespace (omit for same namespace)
  #       tcpPorts: [5432]                   # Optional: List of TCP ports
  #     - name: redis
  #       tcpPorts: [6379]
  #     - name: external-api
  #       labelKey: app.kubernetes.io/name
  #       namespace: external-services
  #       httpPorts: [443, 80]
  #       udpPorts: []
  # Note: DNS egress to kube-system (port 53 UDP/TCP) is automatically included when egress is enabled

statefulset:
  enabled: false
  volume:
    name: data
    mountPath: /data
    storage: 1Gi

dbMigrationConfig:
  enabled: false

command:
  enabled: false
  value: []

args:
  enabled: false
  value: []

resources: {}
  # We usually recommend not to specify default resources and to leave this as a conscious
  # choice for the user. This also increases chances charts run on environments with little
  # resources, such as Minikube. If you do want to specify resources, uncomment the following
  # lines, adjust them as necessary, and remove the curly braces after 'resources:'.

volumeMounts: []
#     - name: log-volume
#       mountPath: /var/log

volumes: []
#     - name: log-volume
#       emptyDir: {}


nodeSelector: {}

# If you need to provide some extra specs for pod which are not included by default in deployment template
# then provide them here
podExtraSpecs: {}

# If you need to provide some extra specs for main container which are not included by default in deployment template
# then provide them here
containerExtraSpecs: {}

# used for deployment algo selection
orchestrator.deploymant.algo: 1

ConfigMaps:
  enabled: false
  maps: []
#  - name: config-map-1
#    type: environment
#    external: false
#    data:
#     key1: key1value-1
#     key2: key2value-1
#     key3: key3value-1
#  - name: config-map-2
#    type: volume
#    external: false
#    mountPath: /etc/config/2
#    data:
#     key1: |
#      club : manchester utd
#      nation : england
#     key2: abc-2
#     key3: abc-2
#  - name: config-map-3
#    type: environment
#    external: true
#    mountPath: /etc/config/3
#    data: []
#  - name: config-map-4
#    type: volume
#    external: true
#    mountPath: /etc/config/4
#    data: []


ConfigSecrets:
  enabled: false
  secrets: []
#  - name: config-secret-1
#    type: environment
#    external: false
#    data:
#     key1: key1value-1
#     key2: key2value-1
#     key3: key3value-1
#  - name: config-secret-2
#    type: volume
#    external: false
#    mountPath: /etc/config/2
#    data:
#     key1: |
#      club : manchester utd
#      nation : england
#     key2: abc-2


initContainers: []
  ## Additional init containers to run before the Scheduler pods.
  ## for example, be used to run a sidecar that chown Logs storage .
  # - name: volume-mount-hack
  #   image: busybox
  #   command: ["sh", "-c", "chown -R 1000:1000 logs"]
  #   volumeMounts:
  #    - mountPath: /usr/local/airflow/logs
  #      name: logs-data
  # # Uncomment below line ONLY IF you want to reuse the container image.
  # # This will assign your application's docker image to init container.
  #   reuseContainerImage: true

containers: []
  ## Additional init containers to run before the Scheduler pods.
  ## for example, be used to run a sidecar that chown Logs storage .
  # - name: volume-mount-hack
  #   image: busybox
  #   command: ["sh", "-c", "chown -R 1000:1000 logs"]
  #   volumeMounts:
  #   - mountPath: /usr/local/airflow/logs
  #     name: logs-data

topologySpreadConstraints:
  - maxSkew: 1
    topologyKey: kubernetes.io/hostname
    whenUnsatisfiable: ScheduleAnyway
    autoLabelSelector: true
    customLabelSelector: {}

envoyproxy:
  image: docker.io/envoyproxy/envoy:v1.16.0
  lifecycle: {}
  configMapName: ""
  resources:
    limits:
      cpu: 50m
      memory: 50Mi
    requests:
      cpu: 50m
      memory: 50Mi

containerSpec:
  lifecycle:
    enabled: false
    preStop: {}
#       exec:
#         command: ["sleep","10"]
    postStart: {}
#       httpGet:
#         host: example.com
#         path: /example
#         port: 90

podDisruptionBudget: {}
#  minAvailable: 1
#  maxUnavailable: 1

  ## Node tolerations for server scheduling to nodes with taints
  ## Ref: https://kubernetes.io/docs/concepts/configuration/assign-pod-node/
  ##

podSecurityContext: {}
  # runAsUser: 1001
  # fsGroup: 1001
  # runAsGroup: 1001

podAnnotations:
  linkerd.io/inject: enabled
  config.linkerd.io/skip-outbound-ports: 3306,5432,5671,6379,26379,9200,587,7233,9042
  config.linkerd.io/proxy-cpu-request: 10m
  config.linkerd.io/proxy-memory-request: 25Mi
  config.linkerd.io/proxy-memory-limit: 75Mi
  config.linkerd.io/init-image: cr.l5d.io/linkerd/proxy-init
  config.linkerd.io/proxy-image: cr.l5d.io/linkerd/proxy

deploymentAnnotations: {}

containerSecurityContext: {}
  # allowPrivilegeEscalation: false
## Pods Service Account
## ref: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
##
serviceAccount:
  ## @param serviceAccount.create Enable creation of ServiceAccount for pods
  ##
  create: true
  ## Enable auto-mounting of kubernetes service account token. Default "false"
  ## Check here for details: https://kubernetes.io/docs/tasks/configure-pod-container/configure-service-account/
  automountServiceAccountToken: false
  ## @param serviceAccount.name The name of the ServiceAccount to use.
  ## If not set and create is true, a name is generated using the `.Chart.Name .fullname` template
  name: ""
  ## @param serviceAccount.annotations Annotations for service account. Evaluated as a template.
  ## Only used if `create` is `true`.
  ##
  annotations: {}

tolerations: []
  #  - key: "key"
  #    operator: "Equal|Exists"
  #    value: "value"
  #    effect: "NoSchedule|PreferNoSchedule|NoExecute(1.6 only)"

imagePullSecrets: []
  # - test1
  # - test2
EnvVariablesFromContainers: []
deploymentType: ROLLING

# -- ServiceProfile configuration for Linkerd
# Enabled by default for production. Disable in staging by setting enabled: false
# NOTE: If rawYaml contains a ServiceProfile, chart will skip generation to avoid conflicts
serviceProfile:
  # -- Enable ServiceProfile resource generation (default: true for production)
  # Set to false in staging/beta deployments or for services with separate serviceprofile.yaml
  enabled: true
  # -- Retry budget configuration (matches existing services)
  retryBudget:
    retryRatio: 0.2
    minRetriesPerSecond: 10
    ttl: 10s
  # -- Route definitions for traffic profiling
  # Default: catch-all route with standard failure detection (400-599)
  # Override with specific routes as needed for detailed API profiling
  routes:
    - name: "all"
      condition:
        pathRegex: /.*
      responseClasses:
        - condition:
            status:
              min: 400
              max: 599
          isFailure: true
